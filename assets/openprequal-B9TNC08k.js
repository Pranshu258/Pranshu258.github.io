import{R as r,j as e}from"./index-DLsh4aVm.js";import{S as i}from"./sharer-D66QtLcw.js";const s="/assets/openprequal-DssVW_Pa.svg";class l extends r.Component{constructor(t){super(t),this.state={currentLatencyMetric:"average",currentDistribution:"prequal"}}componentDidMount(){window.scrollTo(0,0),document.title="Coding YouTube's load balancer using GitHub Copilot | blog by Pranshu Gupta"}render(){return e.jsxs("div",{children:[e.jsx("div",{className:"row bhead",children:e.jsx("i",{className:"fas fa-hexagon-nodes bigger gt1"})}),e.jsx("h1",{className:"title",children:"Coding YouTube's load balancer using GitHub Copilot"}),e.jsx("p",{children:"Pranshu Gupta, September 14, 2025"}),e.jsx(i,{className:"sharer",link:window.location.href,title:"Coding YouTube's load balancer using GitHub Copilot"}),e.jsx("h2",{children:"Introduction"}),e.jsxs("p",{children:["YouTube is the world's largest video sharing platform, where anyone can upload, watch and share videos for free. Over 100 hours of video content is uploaded every minute, a mind bending amount of content, ranging from adorable cats to deep scientific explainations. Load balancing is a critical component of any highly scalable service like YouTube. Google uses an algorithm that it calls ",e.jsx("b",{children:"Prequal"}),', an abbreviation for "Probing to reduce Queueing and Latency", for load balacing services that make up the YouTube platform.']}),e.jsx("p",{children:"Prequal is a load balancer for distributed multi-tenant systems, that aims to minimize real-time request latency in presence of heterogenous server capacities and non-uniform, time-varying antagonist load. In this article, we will explore an AI assisted implementation of this algorithm using Python's FastAPI. FastAPI is a popular web framework for building APIs with Python and has been optimized to be on par with NodeJS and Go (using libraries like Starlette and Pydantic)."}),e.jsx("p",{style:{backgroundColor:"orange",padding:"10px",borderRadius:"5px"},children:"OpenPrequal is an experimental, personal project. FastAPI is very performant, but a Go implementation could still offer lower latency and higher throughput. With additional engineering effort, the Python version here can also be further optimized."}),e.jsx("a",{target:"_blank",rel:"noopener noreferrer",style:{color:"black",textDecoration:"none",marginRight:"10px"},href:"https://github.com/Pranshu258/OpenPrequal",children:e.jsxs("button",{className:"btn btn-danger",children:[e.jsx("i",{className:"fab fa-google"}),e.jsx("b",{style:{padding:"10px"},children:"Google's Prequal Paper"}),e.jsx("i",{className:"fas fa-external-link-alt"})]})}),e.jsx("a",{target:"_blank",rel:"noopener noreferrer",style:{color:"black",textDecoration:"none"},href:"https://github.com/Pranshu258/OpenPrequal",children:e.jsxs("button",{className:"btn btn-primary",children:[e.jsx("i",{className:"fab fa-github"}),e.jsx("b",{style:{padding:"10px"},children:"OpenPrequal on GitHub"}),e.jsx("i",{className:"fas fa-external-link-alt"})]})}),e.jsx("hr",{style:{backgroundColor:"white"}}),e.jsx("h2",{children:"Load Balancing using Reverse Proxy"}),e.jsx("p",{children:"A load balancer is a system that acts as a traffic proxy and disibutes network or application traffic across endpoints on a number of severs. This helps increase the overall performance and availability of applications by reducing the burden on idividual services and distributing deamnd across different surfaces. There are many load balacing algorithms that can be used, each having there pros and cons."}),e.jsxs("ul",{children:[e.jsxs("li",{children:[e.jsx("b",{children:"Round Robin:"})," Distributes requests sequentially across all servers in the pool, ensuring each server receives an equal share of traffic."]}),e.jsxs("li",{children:[e.jsx("b",{children:"Least Connections:"})," Routes new requests to the server with the fewest active connections, balancing the load based on current usage."]}),e.jsxs("li",{children:[e.jsx("b",{children:"Weighted Round Robin:"})," Assigns more requests to servers with higher capacity by giving them a greater weight in the rotation."]}),e.jsxs("li",{children:[e.jsx("b",{children:"Random:"})," Selects a server at random for each request, providing a simple but sometimes uneven distribution."]}),e.jsxs("li",{children:[e.jsx("b",{children:"Least Latency:"})," Directs traffic to the server with the lowest average response time, optimizing for speed."]}),e.jsxs("li",{children:[e.jsx("b",{children:"Power of Two Choices (P2C):"})," Randomly selects two servers and routes the request to the one with fewer active connections (or lower latency), combining randomness with load awareness."]}),e.jsxs("li",{children:[e.jsx("b",{children:"Power of N Choices:"})," Extends P2C by randomly sampling N servers and choosing the best among them (e.g., least loaded or lowest latency), further improving load distribution in large clusters."]})]}),e.jsx("h2",{children:"Probing to reduce Queueing and Latency"}),e.jsx("p",{children:"Prequal (Probing to reduce Queueing and Latency) is a load balacing algorithm that actively probes server load to leverage the power of N choices paradigm, extending it with asynchronous and reusable probes. It does not balance CPU load, but selects servers according to estimated latency and active requests in flight instead. Latency on the server side is defined as the time duration between the application logic receiving the request and handing the response back. The request contributes to the number of requests in flight for server during this time duration."}),e.jsxs("figure",{children:[e.jsx("img",{alt:"",className:"img-fluid",src:s}),e.jsx("figcaption",{children:"OpenPrequal Reverse Proxy and Load Balancer"})]}),e.jsx("p",{children:"The diagram above shows the high level architecture of the OpenPrequal API gateway and load balancer. The components in green are specific to the prequal load balacing algorithm and are not triggered if a different algorithm is configured for the reverse proxy gateway. In the following sections we will discuss each component in detail."}),e.jsx("p",{children:e.jsx("a",{target:"_blank",rel:"noopener noreferrer",style:{color:"black",textDecoration:"none"},href:"https://github.com/Pranshu258/OpenPrequal/blob/main/src/algorithms/prequal_load_balancer.py",children:e.jsxs("button",{className:"btn btn-secondary",children:[e.jsx("i",{className:"fab fa-github"}),e.jsx("b",{style:{padding:"10px"},children:"Prequal Implementation"}),e.jsx("i",{className:"fas fa-external-link-alt"})]})})}),e.jsx("h3",{children:"Probe Management"}),e.jsxs("ul",{children:[e.jsx("li",{children:"The reverse proxy issues a specified number of probes 'r' triggered by each request, in addition to a issuing a forced probe after a configured idle time has been exceeded, to ensure availability of recent probe responses in the pool even when no requests have arrived recently. The probing rate (probes per unit time) is proportional to the ratio of 'r' and incoming requests per second. This ensures that the probing rate remains constant irrespective of the request rate. This is intentional so that the proxy can make decisions based on the latest data, without flooding the backends with probes."}),e.jsx("li",{children:"Probe destinations are sampled uniformly without replacement from the set of available servers. It also helps avoid the thundering herd phenomenon, in which a server with low estimated latency is inundatred with requests as it is seen as the best choice, which leads to request queueing and higher latency."}),e.jsx("li",{children:"When responding to a probe, the RIF comes from simply checking a counter. The estimated latency is the median of the recent latencies observed at the current RIF value. The server maintains a recent history of latency binned over RIF values."}),e.jsx("li",{children:"The proxy maintains a pool of probe responses to be used in server selection. Each pool element indicates the replica server that responded, the timestamp, and the load signals, i.e. current RIF and estimated latency. The pool is capped at a maximum size of 16."})]}),e.jsx("p",{children:e.jsx("a",{target:"_blank",rel:"noopener noreferrer",style:{color:"black",textDecoration:"none"},href:"https://github.com/Pranshu258/OpenPrequal/blob/main/src/core/probe_manager.py",children:e.jsxs("button",{className:"btn btn-secondary",children:[e.jsx("i",{className:"fab fa-github"}),e.jsx("b",{style:{padding:"10px"},children:"Probe Manager Implementation"}),e.jsx("i",{className:"fas fa-external-link-alt"})]})})}),e.jsx("h3",{children:"Metrics Management"}),e.jsxs("ul",{children:[e.jsx("li",{children:"Each server tracks the number of requests in flight and latency statistics, which are provided to the proxy on probe requests."}),e.jsx("li",{children:"The latency is defined as the time duration between the moment when the application logic receives the request and the moment when it forwards the response to the proxy. "}),e.jsx("li",{children:'The request contributes one unit to the "requests in flight" metric during the duration which spans its latency (as described above).'}),e.jsx("li",{children:"When responding to a probe, the RIF comes from simply checking the counter. The latency is always recorded with the RIF at the time the request arrived. The latency metric in the probe response is the median of the recent latencies associated with the current RIF value. If the current value is not available is history, the median is estimated using the closest RIF value. OpenPrequal implementation also does interpolation between two closest available values if possible."})]}),e.jsx("p",{children:e.jsx("a",{target:"_blank",rel:"noopener noreferrer",style:{color:"black",textDecoration:"none"},href:"https://github.com/Pranshu258/OpenPrequal/blob/main/src/core/metrics_manager.py",children:e.jsxs("button",{className:"btn btn-secondary",children:[e.jsx("i",{className:"fab fa-github"}),e.jsx("b",{style:{padding:"10px"},children:"Metrics Manager Implementation"}),e.jsx("i",{className:"fas fa-external-link-alt"})]})})}),e.jsx("h3",{children:"Heartbeat Client"}),e.jsx("p",{children:"The heartbeat client is a module within the backend and it is responsible for sending periodic heartbeats to the proxy server. The heartbeat request includes the current metrics state of the backend. The metrics state is used to update the backend state in the registry. The proxy maintains the timestamps for the most recent heartbeat and marks the backend unhealthy if the heartbeat is older than a configured threshold. Unhealthy backends are not used by the load balancer algorithm for request handling. In Prequal load balancer, probes also update the backend state, in addition to the heartbeats."}),e.jsx("p",{children:e.jsx("a",{target:"_blank",rel:"noopener noreferrer",style:{color:"black",textDecoration:"none"},href:"https://github.com/Pranshu258/OpenPrequal/blob/main/src/core/heartbeat_client.py",children:e.jsxs("button",{className:"btn btn-secondary",children:[e.jsx("i",{className:"fab fa-github"}),e.jsx("b",{style:{padding:"10px"},children:"Heartbeat Client Implementation"}),e.jsx("i",{className:"fas fa-external-link-alt"})]})})}),e.jsx("h3",{children:"Backend Registry"}),e.jsx("p",{children:"The backend registry is the component that maintains the backend server states, including health, along with recent latencies and requests in flight obtained from probes and heartbeats. OpenPrequal supports both in-memory and redis backend registry. The in-memory registry should only be used with single uvicorn workers, because each worker will have its own view of the registry and it's own probe tasks, which might not capture the metrics across all backend workers. However, Redis based backend registry centralizes the backend state, so that all workers have a consistent view, making it suitable for multiple workers for both proxy server and backend servers."}),e.jsx("p",{children:"Multiple workers with in-memory registry is problematic because the backends might not register with some of the workers, resulting in failures if such workers handle the incoming request. This happens because the backend registration process in OpenPrequal is based on heartbeats, not configuration (unlike some other load balancing systems, such as YARP)."}),e.jsxs("p",{children:[e.jsx("a",{target:"_blank",rel:"noopener noreferrer",style:{color:"black",textDecoration:"none",marginRight:"10px"},href:"https://github.com/Pranshu258/OpenPrequal/blob/main/src/core/backend_registry.py",children:e.jsxs("button",{className:"btn btn-secondary",children:[e.jsx("i",{className:"fab fa-github"}),e.jsx("b",{style:{padding:"10px"},children:"In Memory Registry Implementation"}),e.jsx("i",{className:"fas fa-external-link-alt"})]})}),e.jsx("a",{target:"_blank",rel:"noopener noreferrer",style:{color:"black",textDecoration:"none"},href:"https://github.com/Pranshu258/OpenPrequal/blob/main/src/core/redis_backend_registry.py",children:e.jsxs("button",{className:"btn btn-secondary",children:[e.jsx("i",{className:"fab fa-github"}),e.jsx("b",{style:{padding:"10px"},children:"Redis Registry Implementation"}),e.jsx("i",{className:"fas fa-external-link-alt"})]})})]}),e.jsx("hr",{style:{backgroundColor:"white"}}),e.jsx("h2",{children:"Benchmarking"}),e.jsx("p",{children:"Locust and Vegeta are two polpular load testing frameworks. I found that for benchmarking load balacing algorithms, where you mainly care about the number of requests per second, Vegeta is much simpler to configure and gives similar results as Locust. Locust is more appropriate when you want to control the number of users and simulate their behavior, rather than the number of requests."}),e.jsx("p",{children:"After extensive testing with Locust, I found that Prequal is not better than round robin and other simpler load balacing algorithms if the number of backend servers is low. It performs marginally better when the system has 100 or more backend servers across which the proxy needs to load balance."}),e.jsxs("table",{className:"development-timeline-table",style:{width:"100%",borderCollapse:"collapse",marginTop:"20px"},children:[e.jsx("caption",{children:"Metrics at 1200 RPS vegeta attack for 5 seconds, with single proxy worker, and 100 single worker heterogeneous backend servers, with local redis registry, on a MacBook Pro M3 Pro (latencies in ms). Vegeta configured as default (30s client timeout). Performance and success rate degrades rapidly with higher RPS. Increasing the number of proxy workers allows higher RPS."}),e.jsx("thead",{children:e.jsxs("tr",{style:{backgroundColor:"#f8f9fa"},children:[e.jsx("th",{style:{padding:"12px",textAlign:"left",border:"1px solid #dee2e6"},children:"Algorithm"}),e.jsx("th",{style:{padding:"12px",textAlign:"left",border:"1px solid #dee2e6"},children:"Average Latency"}),e.jsx("th",{style:{padding:"12px",textAlign:"left",border:"1px solid #dee2e6"},children:"P50 Latency"}),e.jsx("th",{style:{padding:"12px",textAlign:"left",border:"1px solid #dee2e6"},children:"P95 Latency"}),e.jsx("th",{style:{padding:"12px",textAlign:"left",border:"1px solid #dee2e6"},children:"P99 Latency"}),e.jsx("th",{style:{padding:"12px",textAlign:"left",border:"1px solid #dee2e6"},children:"Max Latency"}),e.jsx("th",{style:{padding:"12px",textAlign:"left",border:"1px solid #dee2e6"},children:"Success Rate"})]})}),e.jsxs("tbody",{children:[e.jsxs("tr",{children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},className:"metric",children:"Least Latency"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"24045"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"26165"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"29706"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"30000"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"30001"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"96.76 %"})]}),e.jsxs("tr",{style:{backgroundColor:"#f8f9fa"},children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},className:"metric",children:"Least Latency (P2C)"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"21883"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"25031"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"27306"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"27437"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"30003"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"95.33 %"})]}),e.jsxs("tr",{children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},className:"metric",children:"Least RIF"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"22455"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"24696"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"28587"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"28985"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"29169"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"100 %"})]}),e.jsxs("tr",{style:{backgroundColor:"#f8f9fa"},children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},className:"metric",children:"Least RIF (P2C)"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"24987"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"27098"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"30000"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"30000"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"30003"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"93.41 %"})]}),e.jsxs("tr",{style:{fontWeight:"bold"},children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},className:"metric",children:"Prequal"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"22975"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"25396"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"27042"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"27209"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"27423"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"100 %"})]}),e.jsxs("tr",{style:{backgroundColor:"#f8f9fa"},children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},className:"metric",children:"Random"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"22806"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"25348"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"27475"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"27612"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"29437"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"99.63 %"})]}),e.jsxs("tr",{children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},className:"metric",children:"Round Robin"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"23522"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"26182"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"28179"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"28442"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"28746"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"99.91 %"})]})]})]}),e.jsx("p",{children:"As we can see in the table above, Prequal and Least RIF perform marginally better than other algorithms under heavy load (similar claim was made by Google in the original paper). However, the performance gap between Prequal and others is modest compared to results shown by Google research team. It could be because of the benchmarking test setup itself and the gaps may widen if we further scale out the backend servers. Also, all the tests I did were done on a single machine, i.e. all the backend servers and the proxy servers were running on the same hardware, which meant that if one of the servers consumed more resources, less were available for others. I tried testing with isolated deployments on Azure, but could not create 100 independent servers due to quota limits for the free tier."}),e.jsxs("p",{children:[e.jsx("a",{target:"_blank",rel:"noopener noreferrer",style:{color:"black",textDecoration:"none",marginRight:"10px"},href:"https://github.com/Pranshu258/OpenPrequal/tree/main/scripts",children:e.jsxs("button",{className:"btn btn-secondary",children:[e.jsx("i",{className:"fab fa-github"}),e.jsx("b",{style:{padding:"10px"},children:"Benchmarking Scripts"}),e.jsx("i",{className:"fas fa-external-link-alt"})]})}),e.jsx("a",{target:"_blank",rel:"noopener noreferrer",style:{color:"black",textDecoration:"none"},href:"https://github.com/Pranshu258/OpenPrequal/tree/main/results",children:e.jsxs("button",{className:"btn btn-secondary",children:[e.jsx("i",{className:"fab fa-github"}),e.jsx("b",{style:{padding:"10px"},children:"More Benchmarking Results"}),e.jsx("i",{className:"fas fa-external-link-alt"})]})})]}),e.jsx("hr",{style:{backgroundColor:"white"}}),e.jsx("h2",{children:"Agent Assisted Coding"}),e.jsx("p",{children:"For this project, I relied entirely on VS Code and GitHub Copilot, guiding the agent by specifying the requirements and desired changes, while making very few manual code edits myself. Most of my prompts were handled by OpenAI's GPT-4.1, with occasional use of GPT-5 mini, o3-mini, and Anthropic Claude Sonnet for tasks where GPT-4.1 did not perform as well. This is expected, since GPT-4.1 is a non-reasoning model and may not match the performance of reasoning models like o3 and GPT-5 on more complex tasks."}),e.jsxs("ul",{children:[e.jsxs("li",{children:[e.jsx("a",{href:"https://platform.openai.com/docs/models/gpt-4.1",children:"OpenAI GPT-4.1"})," excels at instruction following and tool calling, with broad knowledge across domains. It features a 1M token context window, and low latency without a reasoning step."]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://platform.openai.com/docs/models/gpt-5",children:"GPT-5"})," is OpenAI's flagship model for coding, reasoning, and agentic tasks across domains. GPT-5 mini is a faster, more cost-efficient version of GPT-5. It's great for well-defined tasks and precise prompts. "]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://platform.openai.com/docs/models/o3",children:"OpenAI o3"})," is a well-rounded and powerful model across domains. It excels at technical writing and instruction-following, and can be used to think through multi-step problems that involve analysis across text, code, and images. o3-mini is a smaller model alternartive to o3."]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://www.anthropic.com/claude/sonnet",children:"Anthropic Claude Sonnet"})," is a hybrid reasoning model and is a powerful choice for agentic coding, and can complete tasks across the entire software development lifecycleâ€”from initial planning to bug fixes, maintenance to large refactors."]})]}),e.jsx("p",{children:"Reasoning models are LLMs that have been fine tuned to break complex problems into smaller steps, employing chain of thought reasoning and other multi-step decision making strategies, before generating the final output. It has been observed that such models perform better at complex tasks that involve mathematical and logical reasoning, such as programming."}),e.jsx("p",{children:"That being said, even reasoning LLMs are not perfect, they would often get stuck into fixing a syntax error, rewriting the file again and again, while making no progress, or sometimes, make it worse. As an AI agent user, it is essential to know what you want to implement, and to be able to understand and verify if the code that was generated is doing what you intended it to do."}),e.jsx("h3",{children:"Workflow"}),e.jsx("p",{children:"This was made possible by more than a thousand prompts across several months. The table below describes the evolution of the project, starting with a simple reverse proxy gateway and ending with a benchmarking setup, with many improvements and bug fixes along the way. Often, the code generated by the model is not optimized for the scenario, and requires iterative refinement and careful review. My workflow typically followed these steps:"}),e.jsxs("ol",{children:[e.jsxs("li",{children:[e.jsx("b",{children:"Define the requirement:"})," Clearly describe the feature or fix needed, often referencing specific files or functions."]}),e.jsxs("li",{children:[e.jsx("b",{children:"Prompt the agent:"})," Use concise, targeted prompts in VS Code, sometimes including code snippets or error messages."]}),e.jsxs("li",{children:[e.jsx("b",{children:"Review the output:"})," Carefully inspect the generated code for correctness, efficiency, and alignment with the project architecture."]}),e.jsxs("li",{children:[e.jsx("b",{children:"Test and debug:"})," Run the code, observe behavior, and provide feedback to the agent for further refinement if needed."]}),e.jsxs("li",{children:[e.jsx("b",{children:"Iterate:"})," Repeat the process, breaking down complex tasks into smaller steps, until the desired outcome is achieved."]})]}),e.jsx("h3",{children:"Timeline"}),e.jsx("p",{children:"Over six phases the project moved from basic reverse-proxy setup, metrics and testing to building an extensible load-balancer and the Prequal algorithm with health monitoring and a backend registry; it was then optimized (off-path probing, caching, concurrency improvements), expanded to support multiple selection strategies and benchmarking, testing and deployment automation, and finally integrated Redis, better health checks, and refined metrics and monitoring."}),e.jsxs("table",{className:"development-timeline-table",style:{width:"100%",borderCollapse:"collapse",marginTop:"20px"},children:[e.jsx("thead",{children:e.jsxs("tr",{style:{backgroundColor:"#f8f9fa",borderBottom:"2px solid #dee2e6"},children:[e.jsx("th",{style:{padding:"12px",textAlign:"left",border:"1px solid #dee2e6"},children:"Phase"}),e.jsx("th",{style:{padding:"12px",textAlign:"left",border:"1px solid #dee2e6"},children:"Timeline"}),e.jsx("th",{style:{padding:"12px",textAlign:"left",border:"1px solid #dee2e6"},children:"Highlights"})]})}),e.jsxs("tbody",{children:[e.jsxs("tr",{children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:e.jsx("strong",{children:"Initial Foundation"})}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"4 months ago"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"Project setup, reverse proxy, basic metrics, tests and docs."})]}),e.jsxs("tr",{style:{backgroundColor:"#f8f9fa"},children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:e.jsx("strong",{children:"Core Load Balancing Development"})}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"5 weeks ago"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"Core load-balancer, probe manager, RIF/latency metrics, Prequal implementation."})]}),e.jsxs("tr",{children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:e.jsx("strong",{children:"Optimization and Refinement"})}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"3-4 weeks ago"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"Performance: off-path probes, caching, concurrency fixes, load-testing tools."})]}),e.jsxs("tr",{style:{backgroundColor:"#f8f9fa"},children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:e.jsx("strong",{children:"Algorithm Expansion"})}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"2-3 weeks ago"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"Added selection strategies (Least Latency, Least RIF, P2C, Random, RR) and analysis."})]}),e.jsxs("tr",{children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:e.jsx("strong",{children:"Production Readiness"})}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"2 weeks ago"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"Docker, profiling, CI/tests, formatting and docs."})]}),e.jsxs("tr",{style:{backgroundColor:"#f8f9fa"},children:[e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:e.jsx("strong",{children:"Further Optimizations"})}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"Last 2 weeks"}),e.jsx("td",{style:{padding:"12px",border:"1px solid #dee2e6",verticalAlign:"top"},children:"Redis registry, improved health checks, lighter TTLs, and benchmarking docs."})]})]})]}),e.jsx("hr",{style:{backgroundColor:"white"}}),e.jsx("h2",{children:"References"}),e.jsxs("ol",{children:[e.jsxs("li",{children:[e.jsx("a",{href:"https://fastapi.tiangolo.com/",children:"Python FastAPI"})," - modern, fast (high-performance), web framework for building APIs with Python"]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://github.com/features/copilot",children:"GitHub Copilot"})," - AI that builds with you"]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://www.ibm.com/think/topics/reasoning-model",children:"What is a reasoning model?"}),"- by Dave Bergmann, IBM"]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://www.ibm.com/think/topics/chain-of-thoughts",children:"Chain of Thoughts"}),"- What is chain of thought (CoT) prompting?"]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://www.anthropic.com/claude/sonnet",children:"Anthropic Claude Sonnet"}),"- Hybrid reasoning model with superior intelligence"]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://platform.openai.com/docs/models",children:"Models"}),"- OpenAI Platform"]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://dotnet.github.io/yarp/index.html",children:"YARP"}),"- Yet Another Reverse Proxy"]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://aws.amazon.com/what-is/load-balancing/",children:"AWS"})," - What is Load Balancing?"]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://github.com/tsenart/vegeta",children:"Vegeta"})," - A versatile HTTP load testing tool"]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://locust.io/",children:"Locust"})," - A modern load testing framework"]}),e.jsxs("li",{children:[e.jsx("a",{href:"https://www.cloudflare.com/learning/performance/what-is-load-balancing/",children:"Cloudflare"})," - What is Load Balancing?"]})]}),e.jsx("hr",{style:{backgroundColor:"white"}})]})}}export{l as default};
